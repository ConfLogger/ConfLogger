Insert a logging statement to the following code using <API>. Format: <Line#X>:# <Statement>:#

Target Code:
<Line#0> void activate(Configuration conf) {
<Line#1> final int intervalSecs = (int) conf.getTimeDuration(
<Line#2> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY,
<Line#3> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_INTERVAL_DEFAULT,
<Line#4> TimeUnit.SECONDS);
<Line#5> checkArgument(intervalSecs >= 0, "Cannot set a negative " +
<Line#6> "value for " + DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY);
<Line#7> 
<Line#8> int blocksPerInterval = conf.getInt(
<Line#9> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY,
<Line#10> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_DEFAULT);
<Line#11> 
<Line#12> final String deprecatedKey =
<Line#13> "dfs.namenode.decommission.nodes.per.interval";
<Line#14> final String strNodes = conf.get(deprecatedKey);
<Line#15> if (strNodes != null) {
<Line#16> ;
<Line#17> }
<Line#18> 
<Line#19> checkArgument(blocksPerInterval > 0,
<Line#20> "Must set a positive value for "
<Line#21> + DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY);
<Line#22> 
<Line#23> final int maxConcurrentTrackedNodes = conf.getInt(
<Line#24> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES,
<Line#25> DFSConfigKeys
<Line#26> .DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT);
<Line#27> checkArgument(maxConcurrentTrackedNodes >= 0, "Cannot set a negative " +
<Line#28> "value for "
<Line#29> + DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES);
<Line#30> 
<Line#31> Class cls = null;
<Line#32> try {
<Line#33> cls = conf.getClass(
<Line#34> DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MONITOR_CLASS,
<Line#35> DatanodeAdminDefaultMonitor.class);
<Line#36> monitor =
<Line#37> (DatanodeAdminMonitorInterface)ReflectionUtils.newInstance(cls, conf);
<Line#38> monitor.setBlockManager(blockManager);
<Line#39> monitor.setNameSystem(namesystem);
<Line#40> monitor.setDatanodeAdminManager(this);
<Line#41> } catch (Exception e) {
<Line#42> throw new RuntimeException("Unable to create the Decommission monitor " +
<Line#43> "from "+cls, e);
<Line#44> }
<Line#45> executor.scheduleWithFixedDelay(monitor, intervalSecs, intervalSecs,
<Line#46> TimeUnit.SECONDS);
<Line#47> 
<Line#48> LOG.debug("Activating DatanodeAdminManager with interval {} seconds, " +
<Line#49> "{} max blocks per interval, " +
<Line#50> "{} max concurrently tracked nodes.", intervalSecs,
<Line#51> blocksPerInterval, maxConcurrentTrackedNodes);
<Line#52> }

Related Context:
Method A:
<Line#0> void setBlockManager(BlockManager bm);
<Line#1> 
Method B:
<Line#0> void setDatanodeAdminManager(DatanodeAdminManager dnm);
<Line#1> 

Relevant Logging Patterns:
Example 1:
File: hdfs__breakHardlinks-183__.json
Code:
<Line#1>{
<Line#2>  final FileIoProvider fileIoProvider=getFileIoProvider();
<Line#3>  final File tmpFile=DatanodeUtil.createFileWithExistsCheck(getVolume(),b,DatanodeUtil.getUnlinkTmpFile(file),fileIoProvider);
<Line#4>  try {
<Line#5>    try (FileInputStream in=fileIoProvider.getFileInputStream(getVolume(),file)){
<Line#6>      try (FileOutputStream out=fileIoProvider.getFileOutputStream(getVolume(),tmpFile)){
<Line#7>        IOUtils.copyBytes(in,out,16 * 1024);
<Line#8>      }
<Line#9>     }
<Line#10>     if (file.length() != tmpFile.length()) {
<Line#11>      throw new IOException("Copy of file " + file + " size "+ file.length()+ " into file "+ tmpFile+ " resulted in a size of "+ tmpFile.length());
<Line#12>    }
<Line#13>    fileIoProvider.replaceFile(getVolume(),tmpFile,file);
<Line#14>  }
<Line#15> catch (  IOException e) {
<Line#16>    if (!fileIoProvider.delete(getVolume(),tmpFile)) {
<Line#17>      DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile);
<Line#18>    }
<Line#19>    throw e;
<Line#20>  }
<Line#21>}
Log: <Line#17>:# DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile):#

Example 2:
File: hdfs__cancelDelegationToken-766__.json
Code:
<Line#1>{
<Line#2>  LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token));
<Line#3>  try {
<Line#4>    token.cancel(conf);
<Line#5>  }
<Line#6> catch (  InterruptedException ie) {
<Line#7>    throw new RuntimeException("caught interrupted",ie);
<Line#8>  }
<Line#9>catch (  RemoteException re) {
<Line#10>    throw re.unwrapRemoteException(InvalidToken.class,AccessControlException.class);
<Line#11>  }
<Line#12>}
Log: <Line#2>:# LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token)):#

Example 3:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#8>:# LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce):#

Example 4:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#11>:# LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode"):#

Example 5:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#15>:# LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode"):#

Insert a logging statement to the following code using <API>. Format: <Line#X>:# <Statement>:#

Target Code:
<Line#0> int maxNotChangedIterations)
<Line#1> throws IOException {
<Line#2> this.nameNodeUri = nameNodeUri;
<Line#3> this.idPath = idPath;
<Line#4> this.targetPaths = targetPaths == null || targetPaths.isEmpty() ? Arrays
<Line#5> .asList(new Path("/")) : targetPaths;
<Line#6> this.maxNotChangedIterations = maxNotChangedIterations;
<Line#7> int getBlocksMaxQps = conf.getInt(
<Line#8> DFSConfigKeys.DFS_NAMENODE_GETBLOCKS_MAX_QPS_KEY,
<Line#9> DFSConfigKeys.DFS_NAMENODE_GETBLOCKS_MAX_QPS_DEFAULT);
<Line#10> if (getBlocksMaxQps > 0) {
<Line#11> this.getBlocksRateLimiter = RateLimiter.create(getBlocksMaxQps);
<Line#12> } else {
<Line#13> this.getBlocksRateLimiter = null;
<Line#14> }
<Line#15> 
<Line#16> this.namenode = NameNodeProxies.createProxy(conf, nameNodeUri,
<Line#17> BalancerProtocols.class, fallbackToSimpleAuth).getProxy();
<Line#18> this.requestToStandby = conf.getBoolean(
<Line#19> DFSConfigKeys.DFS_HA_ALLOW_STALE_READ_KEY,
<Line#20> DFSConfigKeys.DFS_HA_ALLOW_STALE_READ_DEFAULT);
<Line#21> this.config = conf;
<Line#22> 
<Line#23> this.fs = (DistributedFileSystem)FileSystem.get(nameNodeUri, conf);
<Line#24> 
<Line#25> final NamespaceInfo namespaceinfo = namenode.versionRequest();
<Line#26> this.blockpoolID = namespaceinfo.getBlockPoolID();
<Line#27> 
<Line#28> final FsServerDefaults defaults = fs.getServerDefaults(new Path("/"));
<Line#29> this.keyManager = new KeyManager(blockpoolID, namenode,
<Line#30> defaults.getEncryptDataTransfer(), conf);
<Line#31> // if it is for test, we do not create the id file
<Line#32> if (checkOtherInstanceRunning) {
<Line#33> out = checkAndMarkRunning();
<Line#34> if (out == null) {
<Line#35> // Exit if there is another one running.
<Line#36> throw new IOException("Another " + name + " is running.");
<Line#37> }
<Line#38> }
<Line#39> }

Relevant Logging Patterns:
Example 1:
File: hdfs__breakHardlinks-183__.json
Code:
<Line#1>{
<Line#2>  final FileIoProvider fileIoProvider=getFileIoProvider();
<Line#3>  final File tmpFile=DatanodeUtil.createFileWithExistsCheck(getVolume(),b,DatanodeUtil.getUnlinkTmpFile(file),fileIoProvider);
<Line#4>  try {
<Line#5>    try (FileInputStream in=fileIoProvider.getFileInputStream(getVolume(),file)){
<Line#6>      try (FileOutputStream out=fileIoProvider.getFileOutputStream(getVolume(),tmpFile)){
<Line#7>        IOUtils.copyBytes(in,out,16 * 1024);
<Line#8>      }
<Line#9>     }
<Line#10>     if (file.length() != tmpFile.length()) {
<Line#11>      throw new IOException("Copy of file " + file + " size "+ file.length()+ " into file "+ tmpFile+ " resulted in a size of "+ tmpFile.length());
<Line#12>    }
<Line#13>    fileIoProvider.replaceFile(getVolume(),tmpFile,file);
<Line#14>  }
<Line#15> catch (  IOException e) {
<Line#16>    if (!fileIoProvider.delete(getVolume(),tmpFile)) {
<Line#17>      DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile);
<Line#18>    }
<Line#19>    throw e;
<Line#20>  }
<Line#21>}
Log: <Line#17>:# DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile):#

Example 2:
File: hdfs__cancelDelegationToken-766__.json
Code:
<Line#1>{
<Line#2>  LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token));
<Line#3>  try {
<Line#4>    token.cancel(conf);
<Line#5>  }
<Line#6> catch (  InterruptedException ie) {
<Line#7>    throw new RuntimeException("caught interrupted",ie);
<Line#8>  }
<Line#9>catch (  RemoteException re) {
<Line#10>    throw re.unwrapRemoteException(InvalidToken.class,AccessControlException.class);
<Line#11>  }
<Line#12>}
Log: <Line#2>:# LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token)):#

Example 3:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#8>:# LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce):#

Example 4:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#11>:# LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode"):#

Example 5:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#15>:# LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode"):#

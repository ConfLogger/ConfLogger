Insert a logging statement to the following code using <API>. Format: <Line#X>:# <Statement>:#

Target Code:
<Line#0> boolean ignoreRetryCache)
<Line#1> throws IOException {
<Line#2> provider = DFSUtil.createKeyProviderCryptoExtension(conf);
<Line#3> LOG.info("KeyProvider: " + provider);
<Line#4> if (conf.getBoolean(DFS_NAMENODE_AUDIT_LOG_ASYNC_KEY,
<Line#5> DFS_NAMENODE_AUDIT_LOG_ASYNC_DEFAULT)) {
<Line#6> LOG.info("Enabling async auditlog");
<Line#7> enableAsyncAuditLog(conf);
<Line#8> }
<Line#9> auditLogWithRemotePort =
<Line#10> conf.getBoolean(DFS_NAMENODE_AUDIT_LOG_WITH_REMOTE_PORT_KEY,
<Line#11> DFS_NAMENODE_AUDIT_LOG_WITH_REMOTE_PORT_DEFAULT);
<Line#12> this.contextFieldSeparator =
<Line#13> conf.get(HADOOP_CALLER_CONTEXT_SEPARATOR_KEY,
<Line#14> HADOOP_CALLER_CONTEXT_SEPARATOR_DEFAULT);
<Line#15> fsLock = new FSNamesystemLock(conf, detailedLockHoldTimeMetrics);
<Line#16> cond = fsLock.newWriteLockCondition();
<Line#17> cpLock = new ReentrantLock();
<Line#18> 
<Line#19> this.fsImage = fsImage;
<Line#20> try {
<Line#21> resourceRecheckInterval = conf.getLong(
<Line#22> DFS_NAMENODE_RESOURCE_CHECK_INTERVAL_KEY,
<Line#23> DFS_NAMENODE_RESOURCE_CHECK_INTERVAL_DEFAULT);
<Line#24> 
<Line#25> this.fsOwner = UserGroupInformation.getCurrentUser();
<Line#26> this.supergroup = conf.get(DFS_PERMISSIONS_SUPERUSERGROUP_KEY,
<Line#27> DFS_PERMISSIONS_SUPERUSERGROUP_DEFAULT);
<Line#28> this.isPermissionEnabled = conf.getBoolean(DFS_PERMISSIONS_ENABLED_KEY,
<Line#29> DFS_PERMISSIONS_ENABLED_DEFAULT);
<Line#30> 
<Line#31> this.isStoragePolicyEnabled =
<Line#32> conf.getBoolean(DFS_STORAGE_POLICY_ENABLED_KEY,
<Line#33> DFS_STORAGE_POLICY_ENABLED_DEFAULT);
<Line#34> this.isStoragePolicySuperuserOnly =
<Line#35> conf.getBoolean(DFS_STORAGE_POLICY_PERMISSIONS_SUPERUSER_ONLY_KEY,
<Line#36> DFS_STORAGE_POLICY_PERMISSIONS_SUPERUSER_ONLY_DEFAULT);
<Line#37> 
<Line#38> this.snapshotDiffReportLimit =
<Line#39> conf.getInt(DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT,
<Line#40> DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT);
<Line#41> 
<Line#42> LOG.info("fsOwner                = " + fsOwner);
<Line#43> LOG.info("supergroup             = " + supergroup);
<Line#44> LOG.info("isPermissionEnabled    = " + isPermissionEnabled);
<Line#45> LOG.info("isStoragePolicyEnabled = " + isStoragePolicyEnabled);
<Line#46> 
<Line#47> // block allocation has to be persisted in HA using a shared edits directory
<Line#48> // so that the standby has up-to-date namespace information
<Line#49> nameserviceId = DFSUtil.getNamenodeNameServiceId(conf);
<Line#50> this.haEnabled = HAUtil.isHAEnabled(conf, nameserviceId);
<Line#51> 
<Line#52> // Sanity check the HA-related config.
<Line#53> if (nameserviceId != null) {
<Line#54> LOG.info("Determined nameservice ID: " + nameserviceId);
<Line#55> }
<Line#56> LOG.info("HA Enabled: " + haEnabled);
<Line#57> if (!haEnabled && HAUtil.usesSharedEditsDir(conf)) {
<Line#58> LOG.warn("Configured NNs:\n" + DFSUtil.nnAddressesAsString(conf));
<Line#59> throw new IOException("Invalid configuration: a shared edits dir " +
<Line#60> "must not be specified if HA is not enabled.");
<Line#61> }
<Line#62> 
<Line#63> // block manager needs the haEnabled initialized
<Line#64> this.blockManager = new BlockManager(this, haEnabled, conf);
<Line#65> this.datanodeStatistics = blockManager.getDatanodeManager().getDatanodeStatistics();
<Line#66> 
<Line#67> // Get the checksum type from config
<Line#68> String checksumTypeStr = conf.get(DFS_CHECKSUM_TYPE_KEY,
<Line#69> DFS_CHECKSUM_TYPE_DEFAULT);
<Line#70> DataChecksum.Type checksumType;
<Line#71> try {
<Line#72> checksumType = DataChecksum.Type.valueOf(checksumTypeStr);
<Line#73> } catch (IllegalArgumentException iae) {
<Line#74> throw new IOException("Invalid checksum type in "
<Line#75> + DFS_CHECKSUM_TYPE_KEY + ": " + checksumTypeStr);
<Line#76> }
<Line#77> 
<Line#78> try {
<Line#79> digest = MessageDigest.getInstance("MD5");
<Line#80> } catch (NoSuchAlgorithmException e) {
<Line#81> throw new IOException("Algorithm 'MD5' not found");
<Line#82> }
<Line#83> 
<Line#84> this.serverDefaults = new FsServerDefaults(
<Line#85> conf.getLongBytes(DFS_BLOCK_SIZE_KEY, DFS_BLOCK_SIZE_DEFAULT),
<Line#86> conf.getInt(DFS_BYTES_PER_CHECKSUM_KEY, DFS_BYTES_PER_CHECKSUM_DEFAULT),
<Line#87> conf.getInt(DFS_CLIENT_WRITE_PACKET_SIZE_KEY, DFS_CLIENT_WRITE_PACKET_SIZE_DEFAULT),
<Line#88> (short) conf.getInt(DFS_REPLICATION_KEY, DFS_REPLICATION_DEFAULT),
<Line#89> conf.getInt(IO_FILE_BUFFER_SIZE_KEY, IO_FILE_BUFFER_SIZE_DEFAULT),
<Line#90> conf.getBoolean(DFS_ENCRYPT_DATA_TRANSFER_KEY, DFS_ENCRYPT_DATA_TRANSFER_DEFAULT),
<Line#91> conf.getLong(FS_TRASH_INTERVAL_KEY, FS_TRASH_INTERVAL_DEFAULT),
<Line#92> checksumType,
<Line#93> conf.getTrimmed(
<Line#94> CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH,
<Line#95> ""),
<Line#96> blockManager.getStoragePolicySuite().getDefaultPolicy().getId());
<Line#97> 
<Line#98> this.maxFsObjects = conf.getLong(DFS_NAMENODE_MAX_OBJECTS_KEY,
<Line#99> DFS_NAMENODE_MAX_OBJECTS_DEFAULT);
<Line#100> 
<Line#101> this.minBlockSize = conf.getLongBytes(
<Line#102> DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY,
<Line#103> DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_DEFAULT);
<Line#104> this.maxBlocksPerFile = conf.getLong(DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY,
<Line#105> DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_DEFAULT);
<Line#106> this.batchedListingLimit = conf.getInt(
<Line#107> DFSConfigKeys.DFS_NAMENODE_BATCHED_LISTING_LIMIT,
<Line#108> DFSConfigKeys.DFS_NAMENODE_BATCHED_LISTING_LIMIT_DEFAULT);
<Line#109> Preconditions.checkArgument(
<Line#110> batchedListingLimit > 0,
<Line#111> DFSConfigKeys.DFS_NAMENODE_BATCHED_LISTING_LIMIT +
<Line#112> " must be greater than zero");
<Line#113> this.numCommittedAllowed = conf.getInt(
<Line#114> DFSConfigKeys.DFS_NAMENODE_FILE_CLOSE_NUM_COMMITTED_ALLOWED_KEY,
<Line#115> DFSConfigKeys.DFS_NAMENODE_FILE_CLOSE_NUM_COMMITTED_ALLOWED_DEFAULT);
<Line#116> 
<Line#117> this.maxCorruptFileBlocksReturn = conf.getInt(
<Line#118> DFSConfigKeys.DFS_NAMENODE_MAX_CORRUPT_FILE_BLOCKS_RETURNED_KEY,
<Line#119> DFSConfigKeys.DFS_NAMENODE_MAX_CORRUPT_FILE_BLOCKS_RETURNED_DEFAULT);
<Line#120> 
<Line#121> this.dtpReplaceDatanodeOnFailure = ReplaceDatanodeOnFailure.get(conf);
<Line#122> 
<Line#123> this.standbyShouldCheckpoint = conf.getBoolean(
<Line#124> DFS_HA_STANDBY_CHECKPOINTS_KEY, DFS_HA_STANDBY_CHECKPOINTS_DEFAULT);
<Line#125> // # edit autoroll threshold is a multiple of the checkpoint threshold
<Line#126> this.editLogRollerThreshold = (long)
<Line#127> (conf.getFloat(
<Line#128> DFS_NAMENODE_EDIT_LOG_AUTOROLL_MULTIPLIER_THRESHOLD,
<Line#129> DFS_NAMENODE_EDIT_LOG_AUTOROLL_MULTIPLIER_THRESHOLD_DEFAULT) *
<Line#130> conf.getLong(
<Line#131> DFS_NAMENODE_CHECKPOINT_TXNS_KEY,
<Line#132> DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT));
<Line#133> this.editLogRollerInterval = conf.getInt(
<Line#134> DFS_NAMENODE_EDIT_LOG_AUTOROLL_CHECK_INTERVAL_MS,
<Line#135> DFS_NAMENODE_EDIT_LOG_AUTOROLL_CHECK_INTERVAL_MS_DEFAULT);
<Line#136> 
<Line#137> this.lazyPersistFileScrubIntervalSec = conf.getInt(
<Line#138> DFS_NAMENODE_LAZY_PERSIST_FILE_SCRUB_INTERVAL_SEC,
<Line#139> DFS_NAMENODE_LAZY_PERSIST_FILE_SCRUB_INTERVAL_SEC_DEFAULT);
<Line#140> 
<Line#141> if (this.lazyPersistFileScrubIntervalSec < 0) {
<Line#142> throw new IllegalArgumentException(
<Line#143> DFS_NAMENODE_LAZY_PERSIST_FILE_SCRUB_INTERVAL_SEC
<Line#144> + " must be zero (for disable) or greater than zero.");
<Line#145> }
<Line#146> 
<Line#147> this.edekCacheLoaderDelay = conf.getInt(
<Line#148> DFSConfigKeys.DFS_NAMENODE_EDEKCACHELOADER_INITIAL_DELAY_MS_KEY,
<Line#149> DFSConfigKeys.DFS_NAMENODE_EDEKCACHELOADER_INITIAL_DELAY_MS_DEFAULT);
<Line#150> this.edekCacheLoaderInterval = conf.getInt(
<Line#151> DFSConfigKeys.DFS_NAMENODE_EDEKCACHELOADER_INTERVAL_MS_KEY,
<Line#152> DFSConfigKeys.DFS_NAMENODE_EDEKCACHELOADER_INTERVAL_MS_DEFAULT);
<Line#153> 
<Line#154> this.leaseRecheckIntervalMs = conf.getLong(
<Line#155> DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_KEY,
<Line#156> DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_DEFAULT);
<Line#157> Preconditions.checkArgument(
<Line#158> leaseRecheckIntervalMs > 0,
<Line#159> DFSConfigKeys.DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_KEY +
<Line#160> " must be greater than zero");
<Line#161> this.maxLockHoldToReleaseLeaseMs = conf.getLong(
<Line#162> DFS_NAMENODE_MAX_LOCK_HOLD_TO_RELEASE_LEASE_MS_KEY,
<Line#163> DFS_NAMENODE_MAX_LOCK_HOLD_TO_RELEASE_LEASE_MS_DEFAULT);
<Line#164> 
<Line#165> // For testing purposes, allow the DT secret manager to be started regardless
<Line#166> // of whether security is enabled.
<Line#167> alwaysUseDelegationTokensForTests = conf.getBoolean(
<Line#168> DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,
<Line#169> DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_DEFAULT);
<Line#170> 
<Line#171> this.dtSecretManager = createDelegationTokenSecretManager(conf);
<Line#172> this.dir = new FSDirectory(this, conf);
<Line#173> this.snapshotManager = new SnapshotManager(conf, dir);
<Line#174> this.cacheManager = new CacheManager(this, conf, blockManager);
<Line#175> // Init ErasureCodingPolicyManager instance.
<Line#176> ErasureCodingPolicyManager.getInstance().init(conf);
<Line#177> this.topConf = new TopConf(conf);
<Line#178> this.auditLoggers = initAuditLoggers(conf);
<Line#179> this.isDefaultAuditLogger = auditLoggers.size() == 1 &&
<Line#180> auditLoggers.get(0) instanceof DefaultAuditLogger;
<Line#181> this.retryCache = ignoreRetryCache ? null : initRetryCache(conf);
<Line#182> Class<? extends INodeAttributeProvider> klass = conf.getClass(
<Line#183> DFS_NAMENODE_INODE_ATTRIBUTES_PROVIDER_KEY,
<Line#184> null, INodeAttributeProvider.class);
<Line#185> if (klass != null) {
<Line#186> inodeAttributeProvider = ReflectionUtils.newInstance(klass, conf);
<Line#187> }
<Line#188> this.maxListOpenFilesResponses = conf.getInt(
<Line#189> DFSConfigKeys.DFS_NAMENODE_LIST_OPENFILES_NUM_RESPONSES,
<Line#190> DFSConfigKeys.DFS_NAMENODE_LIST_OPENFILES_NUM_RESPONSES_DEFAULT
<Line#191> );
<Line#192> Preconditions.checkArgument(maxListOpenFilesResponses > 0,
<Line#193> DFSConfigKeys.DFS_NAMENODE_LIST_OPENFILES_NUM_RESPONSES +
<Line#194> " must be a positive integer."
<Line#195> );
<Line#196> this.allowOwnerSetQuota = conf.getBoolean(
<Line#197> DFSConfigKeys.DFS_PERMISSIONS_ALLOW_OWNER_SET_QUOTA_KEY,
<Line#198> DFSConfigKeys.DFS_PERMISSIONS_ALLOW_OWNER_SET_QUOTA_DEFAULT);
<Line#199> this.blockDeletionIncrement = conf.getInt(
<Line#200> DFSConfigKeys.DFS_NAMENODE_BLOCK_DELETION_INCREMENT_KEY,
<Line#201> DFSConfigKeys.DFS_NAMENODE_BLOCK_DELETION_INCREMENT_DEFAULT);
<Line#202> Preconditions.checkArgument(blockDeletionIncrement > 0,
<Line#203> DFSConfigKeys.DFS_NAMENODE_BLOCK_DELETION_INCREMENT_KEY +
<Line#204> " must be a positive integer.");
<Line#205> } catch(IOException e) {
<Line#206> LOG.error(getClass().getSimpleName() + " initialization failed.", e);
<Line#207> close();
<Line#208> throw e;
<Line#209> } catch (RuntimeException re) {
<Line#210> LOG.error(getClass().getSimpleName() + " initialization failed.", re);
<Line#211> close();
<Line#212> throw re;
<Line#213> }
<Line#214> }

Relevant Logging Patterns:
Example 1:
File: hdfs__breakHardlinks-183__.json
Code:
<Line#1>{
<Line#2>  final FileIoProvider fileIoProvider=getFileIoProvider();
<Line#3>  final File tmpFile=DatanodeUtil.createFileWithExistsCheck(getVolume(),b,DatanodeUtil.getUnlinkTmpFile(file),fileIoProvider);
<Line#4>  try {
<Line#5>    try (FileInputStream in=fileIoProvider.getFileInputStream(getVolume(),file)){
<Line#6>      try (FileOutputStream out=fileIoProvider.getFileOutputStream(getVolume(),tmpFile)){
<Line#7>        IOUtils.copyBytes(in,out,16 * 1024);
<Line#8>      }
<Line#9>     }
<Line#10>     if (file.length() != tmpFile.length()) {
<Line#11>      throw new IOException("Copy of file " + file + " size "+ file.length()+ " into file "+ tmpFile+ " resulted in a size of "+ tmpFile.length());
<Line#12>    }
<Line#13>    fileIoProvider.replaceFile(getVolume(),tmpFile,file);
<Line#14>  }
<Line#15> catch (  IOException e) {
<Line#16>    if (!fileIoProvider.delete(getVolume(),tmpFile)) {
<Line#17>      DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile);
<Line#18>    }
<Line#19>    throw e;
<Line#20>  }
<Line#21>}
Log: <Line#17>:# DataNode.LOG.info("detachFile failed to delete temporary file " + tmpFile):#

Example 2:
File: hdfs__cancelDelegationToken-766__.json
Code:
<Line#1>{
<Line#2>  LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token));
<Line#3>  try {
<Line#4>    token.cancel(conf);
<Line#5>  }
<Line#6> catch (  InterruptedException ie) {
<Line#7>    throw new RuntimeException("caught interrupted",ie);
<Line#8>  }
<Line#9>catch (  RemoteException re) {
<Line#10>    throw re.unwrapRemoteException(InvalidToken.class,AccessControlException.class);
<Line#11>  }
<Line#12>}
Log: <Line#2>:# LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token)):#

Example 3:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#8>:# LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce):#

Example 4:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#11>:# LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode"):#

Example 5:
File: hdfs__verifyChunks-484__.json
Code:
<Line#1>{
<Line#2>  try {
<Line#3>    clientChecksum.verifyChunkedSums(dataBuf,checksumBuf,clientname,0);
<Line#4>  }
<Line#5> catch (  ChecksumException ce) {
<Line#6>    PacketHeader header=packetReceiver.getHeader();
<Line#7>    String specificOffset="specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = "+ ce.getPos();
<Line#8>    LOG.warn("Checksum error in block " + block + " from "+ inAddr+ ", "+ specificOffset,ce);
<Line#9>    if (srcDataNode != null && isDatanode) {
<Line#10>      try {
<Line#11>        LOG.info("report corrupt " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#12>        datanode.reportRemoteBadBlock(srcDataNode,block);
<Line#13>      }
<Line#14> catch (      IOException e) {
<Line#15>        LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode");
<Line#16>      }
<Line#17>    }
<Line#18>    throw new IOException("Unexpected checksum mismatch while writing " + block + " from "+ inAddr);
<Line#19>  }
<Line#20>}
Log: <Line#15>:# LOG.warn("Failed to report bad " + block + " from datanode "+ srcDataNode+ " to namenode"):#

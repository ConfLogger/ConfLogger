Insert a logging statement to the following code using <API>. Format: <Line#X>:# <Statement>:#

Target Code:
<Line#0> IOException {
<Line#1> if (isFirstAttempt()) {
<Line#2> return false;  // no need to recover on the first attempt
<Line#3> }
<Line#4> 
<Line#5> boolean recoveryEnabled = getConfig().getBoolean(
<Line#6> MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE,
<Line#7> MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);
<Line#8> if (!recoveryEnabled) {
<Line#9> return false;
<Line#10> }
<Line#11> 
<Line#12> boolean recoverySupportedByCommitter = isRecoverySupported();
<Line#13> if (!recoverySupportedByCommitter) {
<Line#14> LOG.info("Not attempting to recover. Recovery is not supported by " +
<Line#15> committer.getClass() + ". Use an OutputCommitter that supports" +
<Line#16> " recovery.");
<Line#17> return false;
<Line#18> }
<Line#19> 
<Line#20> int reducerCount = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);
<Line#21> 
<Line#22> // If a shuffle secret was not provided by the job client, one will be
<Line#23> // generated in this job attempt. However, that disables recovery if
<Line#24> // there are reducers as the shuffle secret would be job attempt specific.
<Line#25> boolean shuffleKeyValidForRecovery =
<Line#26> TokenCache.getShuffleSecretKey(jobCredentials) != null;
<Line#27> if (reducerCount > 0 && !shuffleKeyValidForRecovery) {
<Line#28> LOG.info("Not attempting to recover. The shuffle key is invalid for " +
<Line#29> "recovery.");
<Line#30> return false;
<Line#31> }
<Line#32> 
<Line#33> // If the intermediate data is encrypted, recovering the job requires the
<Line#34> // access to the key. Until the encryption key is persisted, we should
<Line#35> // avoid attempts to recover.
<Line#36> boolean spillEncrypted = CryptoUtils.isEncryptedSpillEnabled(getConfig());
<Line#37> if (reducerCount > 0 && spillEncrypted) {
<Line#38> LOG.info("Not attempting to recover. Intermediate spill encryption" +
<Line#39> " is enabled.");
<Line#40> return false;
<Line#41> }
<Line#42> 
<Line#43> return true;
<Line#44> }

Related Context:
Method A:
<Line#0> private JobContext getJobContextFromConf(Configuration conf){
<Line#1>   if (newApiCommitter) {
<Line#2>     return new JobContextImpl(conf,TypeConverter.fromYarn(getJobId()));
<Line#3>   }
<Line#4>  else {
<Line#5>     return new org.apache.hadoop.mapred.JobContextImpl(new JobConf(conf),TypeConverter.fromYarn(getJobId()));
<Line#6>   }
<Line#7> }
<Line#8> 
Method B:
<Line#0> /** 
<Line#1>  * auxiliary method to get user's secret keys..
<Line#2>  * @param alias
<Line#3>  * @return secret key from the storage
<Line#4>  */
<Line#5> public static byte[] getSecretKey(Credentials credentials,Text alias){
<Line#6>   if (credentials == null)   return null;
<Line#7>   return credentials.getSecretKey(alias);
<Line#8> }
<Line#9> 

Relevant Logging Patterns:
Example 1:
File: mapreduce__createNativeObject-81__.json
Code:
<Line#1>{
<Line#2>  assertNativeLibraryLoaded();
<Line#3>  final long ret=JNICreateNativeObject(clazz.getBytes(Charsets.UTF_8));
<Line#4>  if (ret == 0) {
<Line#5>    LOG.warn("Can't create NativeObject for class " + clazz + ", probably not exist.");
<Line#6>  }
<Line#7>  return ret;
<Line#8>}
Log: <Line#5>:# LOG.warn("Can't create NativeObject for class " + clazz + ", probably not exist."):#

Example 2:
File: mapreduce__createControlFile-122__.json
Code:
<Line#1>{
<Line#2>  LOG.info("creating control file: " + fileSize + " mega bytes, "+ nrFiles+ " files");
<Line#3>  fs.delete(CONTROL_DIR,true);
<Line#4>  for (int i=0; i < nrFiles; i++) {
<Line#5>    String name=getFileName(i);
<Line#6>    Path controlFile=new Path(CONTROL_DIR,"in_file_" + name);
<Line#7>    SequenceFile.Writer writer=null;
<Line#8>    try {
<Line#9>      writer=SequenceFile.createWriter(fs,fsConfig,controlFile,Text.class,LongWritable.class,CompressionType.NONE);
<Line#10>      writer.append(new Text(name),new LongWritable(fileSize));
<Line#11>    }
<Line#12> catch (    Exception e) {
<Line#13>      throw new IOException(e.getLocalizedMessage());
<Line#14>    }
<Line#15> finally {
<Line#16>      if (writer != null)       writer.close();
<Line#17>      writer=null;
<Line#18>    }
<Line#19>  }
<Line#20>  LOG.info("created control files for: " + nrFiles + " files");
<Line#21>}
Log: <Line#2>:# LOG.info("creating control file: " + fileSize + " mega bytes, "+ nrFiles+ " files"):#

Example 3:
File: mapreduce__createControlFile-122__.json
Code:
<Line#1>{
<Line#2>  LOG.info("creating control file: " + fileSize + " mega bytes, "+ nrFiles+ " files");
<Line#3>  fs.delete(CONTROL_DIR,true);
<Line#4>  for (int i=0; i < nrFiles; i++) {
<Line#5>    String name=getFileName(i);
<Line#6>    Path controlFile=new Path(CONTROL_DIR,"in_file_" + name);
<Line#7>    SequenceFile.Writer writer=null;
<Line#8>    try {
<Line#9>      writer=SequenceFile.createWriter(fs,fsConfig,controlFile,Text.class,LongWritable.class,CompressionType.NONE);
<Line#10>      writer.append(new Text(name),new LongWritable(fileSize));
<Line#11>    }
<Line#12> catch (    Exception e) {
<Line#13>      throw new IOException(e.getLocalizedMessage());
<Line#14>    }
<Line#15> finally {
<Line#16>      if (writer != null)       writer.close();
<Line#17>      writer=null;
<Line#18>    }
<Line#19>  }
<Line#20>  LOG.info("created control files for: " + nrFiles + " files");
<Line#21>}
Log: <Line#20>:# LOG.info("created control files for: " + nrFiles + " files"):#

Example 4:
File: mapreduce__addMap-1106__.json
Code:
<Line#1>{
<Line#2>  ContainerRequest request=null;
<Line#3>  if (event.getEarlierAttemptFailed()) {
<Line#4>    earlierFailedMaps.add(event.getAttemptID());
<Line#5>    request=new ContainerRequest(event,PRIORITY_FAST_FAIL_MAP,mapNodeLabelExpression);
<Line#6>    LOG.info("Added " + event.getAttemptID() + " to list of failed maps");
<Line#7>    maps.put(event.getAttemptID(),request);
<Line#8>    addContainerReq(request);
<Line#9>  }
<Line#10> else {
<Line#11>    if (mapsMod100 < numOpportunisticMapsPercent) {
<Line#12>      request=new ContainerRequest(event,PRIORITY_OPPORTUNISTIC_MAP,mapNodeLabelExpression);
<Line#13>      maps.put(event.getAttemptID(),request);
<Line#14>      addOpportunisticResourceRequest(request.priority,request.capability);
<Line#15>    }
<Line#16> else {
<Line#17>      request=new ContainerRequest(event,PRIORITY_MAP,mapNodeLabelExpression);
<Line#18>      for (      String host : event.getHosts()) {
<Line#19>        LinkedList<TaskAttemptId> list=mapsHostMapping.get(host);
<Line#20>        if (list == null) {
<Line#21>          list=new LinkedList<TaskAttemptId>();
<Line#22>          mapsHostMapping.put(host,list);
<Line#23>        }
<Line#24>        list.add(event.getAttemptID());
<Line#25>        if (LOG.isDebugEnabled()) {
<Line#26>          LOG.debug("Added attempt req to host " + host);
<Line#27>        }
<Line#28>      }
<Line#29>      for (      String rack : event.getRacks()) {
<Line#30>        LinkedList<TaskAttemptId> list=mapsRackMapping.get(rack);
<Line#31>        if (list == null) {
<Line#32>          list=new LinkedList<TaskAttemptId>();
<Line#33>          mapsRackMapping.put(rack,list);
<Line#34>        }
<Line#35>        list.add(event.getAttemptID());
<Line#36>        if (LOG.isDebugEnabled()) {
<Line#37>          LOG.debug("Added attempt req to rack " + rack);
<Line#38>        }
<Line#39>      }
<Line#40>      maps.put(event.getAttemptID(),request);
<Line#41>      addContainerReq(request);
<Line#42>    }
<Line#43>    mapsMod100++;
<Line#44>    mapsMod100%=100;
<Line#45>  }
<Line#46>}
Log: <Line#6>:# LOG.info("Added " + event.getAttemptID() + " to list of failed maps"):#

Example 5:
File: mapreduce__addMap-1106__.json
Code:
<Line#1>{
<Line#2>  ContainerRequest request=null;
<Line#3>  if (event.getEarlierAttemptFailed()) {
<Line#4>    earlierFailedMaps.add(event.getAttemptID());
<Line#5>    request=new ContainerRequest(event,PRIORITY_FAST_FAIL_MAP,mapNodeLabelExpression);
<Line#6>    LOG.info("Added " + event.getAttemptID() + " to list of failed maps");
<Line#7>    maps.put(event.getAttemptID(),request);
<Line#8>    addContainerReq(request);
<Line#9>  }
<Line#10> else {
<Line#11>    if (mapsMod100 < numOpportunisticMapsPercent) {
<Line#12>      request=new ContainerRequest(event,PRIORITY_OPPORTUNISTIC_MAP,mapNodeLabelExpression);
<Line#13>      maps.put(event.getAttemptID(),request);
<Line#14>      addOpportunisticResourceRequest(request.priority,request.capability);
<Line#15>    }
<Line#16> else {
<Line#17>      request=new ContainerRequest(event,PRIORITY_MAP,mapNodeLabelExpression);
<Line#18>      for (      String host : event.getHosts()) {
<Line#19>        LinkedList<TaskAttemptId> list=mapsHostMapping.get(host);
<Line#20>        if (list == null) {
<Line#21>          list=new LinkedList<TaskAttemptId>();
<Line#22>          mapsHostMapping.put(host,list);
<Line#23>        }
<Line#24>        list.add(event.getAttemptID());
<Line#25>        if (LOG.isDebugEnabled()) {
<Line#26>          LOG.debug("Added attempt req to host " + host);
<Line#27>        }
<Line#28>      }
<Line#29>      for (      String rack : event.getRacks()) {
<Line#30>        LinkedList<TaskAttemptId> list=mapsRackMapping.get(rack);
<Line#31>        if (list == null) {
<Line#32>          list=new LinkedList<TaskAttemptId>();
<Line#33>          mapsRackMapping.put(rack,list);
<Line#34>        }
<Line#35>        list.add(event.getAttemptID());
<Line#36>        if (LOG.isDebugEnabled()) {
<Line#37>          LOG.debug("Added attempt req to rack " + rack);
<Line#38>        }
<Line#39>      }
<Line#40>      maps.put(event.getAttemptID(),request);
<Line#41>      addContainerReq(request);
<Line#42>    }
<Line#43>    mapsMod100++;
<Line#44>    mapsMod100%=100;
<Line#45>  }
<Line#46>}
Log: <Line#26>:# LOG.debug("Added attempt req to host " + host):#

select <line#> and insert log level and Log verbosity message after <line#>

Query: Target method code:
<Line#0>public static Collection<InMemoryAliasMapProtocol> init(Configuration conf) {
<Line#1>Collection<InMemoryAliasMapProtocol> aliasMaps = new ArrayList<>();
<Line#2>// Try to connect to all configured nameservices as it is not known which
<Line#3>// nameservice supports the AliasMap.
<Line#4>for (String nsId : getNameServiceIds(conf)) {
<Line#5>try {
<Line#6>URI namenodeURI = null;
<Line#7>Configuration newConf = new Configuration(conf);
<Line#8>if (HAUtil.isHAEnabled(conf, nsId)) {
<Line#9>// set the failover-proxy provider if HA is enabled.
<Line#10>newConf.setClass(
<Line#11>addKeySuffixes(PROXY_PROVIDER_KEY_PREFIX, nsId),
<Line#12>InMemoryAliasMapFailoverProxyProvider.class,
<Line#13>AbstractNNFailoverProxyProvider.class);
<Line#14>namenodeURI = new URI(HdfsConstants.HDFS_URI_SCHEME + "://" + nsId);
<Line#15>} else {
<Line#16>String key =
<Line#17>addKeySuffixes(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS, nsId);
<Line#18>String addr = conf.get(key);
<Line#19>if (addr != null) {
<Line#20>namenodeURI = createUri(HdfsConstants.HDFS_URI_SCHEME,
<Line#21>NetUtils.createSocketAddr(addr));
<Line#22>}
<Line#23>}
<Line#24>if (namenodeURI != null) {
<Line#25>aliasMaps.add(NameNodeProxies
<Line#26>.createProxy(newConf, namenodeURI, InMemoryAliasMapProtocol.class)
<Line#27>.getProxy());
<Line#28>}
<Line#29>} catch (IOException | URISyntaxException e) {
<Line#30>LOG.warn("Exception in connecting to InMemoryAliasMap for nameservice "
<Line#31>+ "{}: {}", nsId, e);
<Line#32>}
<Line#33>}
<Line#34>// if a separate AliasMap is configured using
<Line#35>// DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS, try to connect it.
<Line#36>if (conf.get(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS) != null) {
<Line#37>URI uri = createUri("hdfs", NetUtils.createSocketAddr(
<Line#38>conf.get(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS)));
<Line#39>try {
<Line#40>aliasMaps.add(NameNodeProxies
<Line#41>.createProxy(conf, uri, InMemoryAliasMapProtocol.class).getProxy());
<Line#42>LOG.info("Connected to InMemoryAliasMap at {}", uri);
<Line#43>} catch (IOException e) {
<Line#44>LOG.warn("Exception in connecting to InMemoryAliasMap at {}: {}", uri,
<Line#45>e);
<Line#46>}
<Line#47>}
<Line#48>return aliasMaps;
<Line#49>}

Example 1:
<Line#1>{
<Line#2>synchronized (dataQueue) {
<Line#3>    try {
<Line#4>      boolean firstWait=true;
<Line#5>      try {
<Line#6>        while (!streamerClosed && dataQueue.size() + ackQueue.size() > dfsClient.getConf().getWriteMaxPackets()) {
<Line#7>          if (firstWait) {
<Line#8>            Span span=Tracer.getCurrentSpan();
<Line#9>            if (span != null) {
<Line#10>              span.addTimelineAnnotation("dataQueue.wait");
<Line#11>            }
<Line#12>            firstWait=false;
<Line#13>          }
<Line#14>          try {
<Line#15>            dataQueue.wait();
<Line#16>          }
<Line#17> catch (          InterruptedException e) {
<Line#18>            Thread.currentThread().interrupt();
<Line#19>            break;
<Line#20>          }
<Line#21>        }
<Line#22>      }
<Line#23>  finally {
<Line#24>        Span span=Tracer.getCurrentSpan();
<Line#25>        if ((span != null) && (!firstWait)) {
<Line#26>          span.addTimelineAnnotation("end.wait");
<Line#27>        }
<Line#28>      }
<Line#29>      checkClosed();
<Line#30>      queuePacket(packet);
<Line#31>    }
<Line#32> catch (    ClosedChannelException cce) {
<Line#33>      LOG.debug("Closed channel exception",cce);
<Line#34>    }
<Line#35>  }
<Line#36>}
Label: <Line#33> LOG.debug("Closed channel exception",cce)

Example 2:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#20> LOG.debug("Thread interrupted",e)

Example 3:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#39> LOG.debug("stage={}, {}",stage,this)

Example 4:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#41> LOG.debug("Allocating new block: {}",this)

Example 5:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#46> LOG.debug("Append to block {}",block)


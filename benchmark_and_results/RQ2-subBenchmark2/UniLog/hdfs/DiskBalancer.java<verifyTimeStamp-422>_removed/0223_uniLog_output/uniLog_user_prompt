select <line#> and insert log level and Log verbosity message after <line#>

Query: Target method code:
<Line#0>private void verifyTimeStamp(NodePlan plan) throws DiskBalancerException {
<Line#1>long now = Time.now();
<Line#2>long planTime = plan.getTimeStamp();
<Line#3>
<Line#4>if ((planTime + planValidityInterval) < now) {
<Line#5>String planValidity = config.get(
<Line#6>DFSConfigKeys.DFS_DISK_BALANCER_PLAN_VALID_INTERVAL,
<Line#7>DFSConfigKeys.DFS_DISK_BALANCER_PLAN_VALID_INTERVAL_DEFAULT);
<Line#8>if (planValidity.matches("[0-9]$")) {
<Line#9>planValidity += "ms";
<Line#10>}
<Line#11>throw new DiskBalancerException(errorString,
<Line#12>DiskBalancerException.Result.OLD_PLAN_SUBMITTED);
<Line#13>}
<Line#14>}

Example 1:
<Line#1>{
<Line#2>  Preconditions.checkArgument(reconstructLength >= 0 && reconstructLength <= bufferSize);
<Line#3>  int nSuccess=0;
<Line#4>  int[] newSuccess=new int[minRequiredSources];
<Line#5>  BitSet usedFlag=new BitSet(sources.length);
<Line#6>  for (int i=0; i < minRequiredSources; i++) {
<Line#7>    StripedBlockReader reader=readers.get(successList[i]);
<Line#8>    int toRead=getReadLength(liveIndices[successList[i]],reconstructLength);
<Line#9>    if (toRead > 0) {
<Line#10>      Callable<BlockReadStats> readCallable=reader.readFromBlock(toRead,corruptedBlocks);
<Line#11>      Future<BlockReadStats> f=readService.submit(readCallable);
<Line#12>      futures.put(f,successList[i]);
<Line#13>    }
<Line#14> else {
<Line#15>      reader.getReadBuffer().position(0);
<Line#16>      newSuccess[nSuccess++]=successList[i];
<Line#17>    }
<Line#18>    usedFlag.set(successList[i]);
<Line#19>  }
<Line#20>  while (!futures.isEmpty()) {
<Line#21>    try {
<Line#22>      StripingChunkReadResult result=StripedBlockUtil.getNextCompletedStripedRead(readService,futures,stripedReadTimeoutInMills);
<Line#23>      int resultIndex=-1;
<Line#24>      if (result.state == StripingChunkReadResult.SUCCESSFUL) {
<Line#25>        resultIndex=result.index;
<Line#26>      }
<Line#27> else       if (result.state == StripingChunkReadResult.FAILED) {
<Line#28>        StripedBlockReader failedReader=readers.get(result.index);
<Line#29>        failedReader.closeBlockReader();
<Line#30>        resultIndex=scheduleNewRead(usedFlag,reconstructLength,corruptedBlocks);
<Line#31>      }
<Line#32> else       if (result.state == StripingChunkReadResult.TIMEOUT) {
<Line#33>        resultIndex=scheduleNewRead(usedFlag,reconstructLength,corruptedBlocks);
<Line#34>      }
<Line#35>      if (resultIndex >= 0) {
<Line#36>        newSuccess[nSuccess++]=resultIndex;
<Line#37>        if (nSuccess >= minRequiredSources) {
<Line#38>          cancelReads(futures.keySet());
<Line#39>          clearFuturesAndService();
<Line#40>          break;
<Line#41>        }
<Line#42>      }
<Line#43>    }
<Line#44> catch (    InterruptedException e) {
<Line#45>      LOG.info("Read data interrupted.",e);
<Line#46>      cancelReads(futures.keySet());
<Line#47>      clearFuturesAndService();
<Line#48>      break;
<Line#49>    }
<Line#50>  }
<Line#51>  if (nSuccess < minRequiredSources) {
<Line#52>    String error="Can't read data from minimum number of sources " + "required by reconstruction, block id: " + reconstructor.getBlockGroup().getBlockId();
<Line#53>    throw new IOException(error);
<Line#54>  }
<Line#55>  return newSuccess;
<Line#56>}
Label: <Line#45> LOG.info("Read data interrupted.",e)

Example 2:
<Line#1>{
<Line#2>  if (LOG.isDebugEnabled()) {
<Line#3>    LOG.debug("logEdit " + edit);
<Line#4>  }
<Line#5>  try {
<Line#6>    if (!editPendingQ.offer(edit)) {
<Line#7>      Preconditions.checkState(isSyncThreadAlive(),"sync thread is not alive");
<Line#8>      long now=Time.monotonicNow();
<Line#9>      if (now - lastFull > 4000) {
<Line#10>        lastFull=now;
<Line#11>        LOG.info("Edit pending queue is full");
<Line#12>      }
<Line#13>      if (Thread.holdsLock(this)) {
<Line#14>        int permits=overflowMutex.drainPermits();
<Line#15>        try {
<Line#16>          do {
<Line#17>            this.wait(1000);
<Line#18>          }
<Line#19> while (!editPendingQ.offer(edit));
<Line#20>        }
<Line#21>  finally {
<Line#22>          overflowMutex.release(permits);
<Line#23>        }
<Line#24>      }
<Line#25> else {
<Line#26>        overflowMutex.acquire();
<Line#27>        try {
<Line#28>          editPendingQ.put(edit);
<Line#29>        }
<Line#30>  finally {
<Line#31>          overflowMutex.release();
<Line#32>        }
<Line#33>      }
<Line#34>    }
<Line#35>  }
<Line#36> catch (  Throwable t) {
<Line#37>    terminate(t);
<Line#38>  }
<Line#39>}
Label: <Line#3> LOG.debug("logEdit " + edit)

Example 3:
<Line#1>{
<Line#2>  if (LOG.isDebugEnabled()) {
<Line#3>    LOG.debug("logEdit " + edit);
<Line#4>  }
<Line#5>  try {
<Line#6>    if (!editPendingQ.offer(edit)) {
<Line#7>      Preconditions.checkState(isSyncThreadAlive(),"sync thread is not alive");
<Line#8>      long now=Time.monotonicNow();
<Line#9>      if (now - lastFull > 4000) {
<Line#10>        lastFull=now;
<Line#11>        LOG.info("Edit pending queue is full");
<Line#12>      }
<Line#13>      if (Thread.holdsLock(this)) {
<Line#14>        int permits=overflowMutex.drainPermits();
<Line#15>        try {
<Line#16>          do {
<Line#17>            this.wait(1000);
<Line#18>          }
<Line#19> while (!editPendingQ.offer(edit));
<Line#20>        }
<Line#21>  finally {
<Line#22>          overflowMutex.release(permits);
<Line#23>        }
<Line#24>      }
<Line#25> else {
<Line#26>        overflowMutex.acquire();
<Line#27>        try {
<Line#28>          editPendingQ.put(edit);
<Line#29>        }
<Line#30>  finally {
<Line#31>          overflowMutex.release();
<Line#32>        }
<Line#33>      }
<Line#34>    }
<Line#35>  }
<Line#36> catch (  Throwable t) {
<Line#37>    terminate(t);
<Line#38>  }
<Line#39>}
Label: <Line#11> LOG.info("Edit pending queue is full")

Example 4:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#20> LOG.debug("Thread interrupted",e)

Example 5:
<Line#1>{
<Line#2>  TraceScope scope=null;
<Line#3>  while (!streamerClosed && dfsClient.clientRunning) {
<Line#4>    if (errorState.hasError()) {
<Line#5>      closeResponder();
<Line#6>    }
<Line#7>    DFSPacket one;
<Line#8>    try {
<Line#9>      boolean doSleep=processDatanodeOrExternalError();
<Line#10>synchronized (dataQueue) {
<Line#11>        while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {
<Line#12>          long timeout=1000;
<Line#13>          if (stage == BlockConstructionStage.DATA_STREAMING) {
<Line#14>            timeout=sendHeartbeat();
<Line#15>          }
<Line#16>          try {
<Line#17>            dataQueue.wait(timeout);
<Line#18>          }
<Line#19> catch (          InterruptedException e) {
<Line#20>            LOG.debug("Thread interrupted",e);
<Line#21>          }
<Line#22>          doSleep=false;
<Line#23>        }
<Line#24>        if (shouldStop()) {
<Line#25>          continue;
<Line#26>        }
<Line#27>        one=dataQueue.getFirst();
<Line#28>        SpanContext[] parents=one.getTraceParents();
<Line#29>        if (parents != null && parents.length > 0) {
<Line#30>          scope=dfsClient.getTracer().newScope("dataStreamer",parents[0],false);
<Line#31>        }
<Line#32>      }
<Line#33>      try {
<Line#34>        backOffIfNecessary();
<Line#35>      }
<Line#36> catch (      InterruptedException e) {
<Line#37>        LOG.debug("Thread interrupted",e);
<Line#38>      }
<Line#39>      LOG.debug("stage={}, {}",stage,this);
<Line#40>      if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
<Line#41>        LOG.debug("Allocating new block: {}",this);
<Line#42>        setPipeline(nextBlockOutputStream());
<Line#43>        initDataStreaming();
<Line#44>      }
<Line#45> else       if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
<Line#46>        LOG.debug("Append to block {}",block);
<Line#47>        setupPipelineForAppendOrRecovery();
<Line#48>        if (streamerClosed) {
<Line#49>          continue;
<Line#50>        }
<Line#51>        initDataStreaming();
<Line#52>      }
<Line#53>      long lastByteOffsetInBlock=one.getLastByteOffsetBlock();
<Line#54>      if (lastByteOffsetInBlock > stat.getBlockSize()) {
<Line#55>        throw new IOException("BlockSize " + stat.getBlockSize() + " < lastByteOffsetInBlock, "+ this+ ", "+ one);
<Line#56>      }
<Line#57>      if (one.isLastPacketInBlock()) {
<Line#58>        waitForAllAcks();
<Line#59>        if (shouldStop()) {
<Line#60>          continue;
<Line#61>        }
<Line#62>        stage=BlockConstructionStage.PIPELINE_CLOSE;
<Line#63>      }
<Line#64>      SpanContext spanContext=null;
<Line#65>synchronized (dataQueue) {
<Line#66>        if (!one.isHeartbeatPacket()) {
<Line#67>          if (scope != null) {
<Line#68>            one.setSpan(scope.span());
<Line#69>            spanContext=scope.span().getContext();
<Line#70>            scope.close();
<Line#71>          }
<Line#72>          scope=null;
<Line#73>          dataQueue.removeFirst();
<Line#74>          ackQueue.addLast(one);
<Line#75>          packetSendTime.put(one.getSeqno(),Time.monotonicNow());
<Line#76>          dataQueue.notifyAll();
<Line#77>        }
<Line#78>      }
<Line#79>      LOG.debug("{} sending {}",this,one);
<Line#80>      try (TraceScope ignored=dfsClient.getTracer().newScope("DataStreamer#writeTo",spanContext)){
<Line#81>        sendPacket(one);
<Line#82>      }
<Line#83> catch (      IOException e) {
<Line#84>        errorState.markFirstNodeIfNotMarked();
<Line#85>        throw e;
<Line#86>      }
<Line#87>      long tmpBytesSent=one.getLastByteOffsetBlock();
<Line#88>      if (bytesSent < tmpBytesSent) {
<Line#89>        bytesSent=tmpBytesSent;
<Line#90>      }
<Line#91>      if (shouldStop()) {
<Line#92>        continue;
<Line#93>      }
<Line#94>      if (one.isLastPacketInBlock()) {
<Line#95>        try {
<Line#96>          waitForAllAcks();
<Line#97>        }
<Line#98> catch (        IOException ioe) {
<Line#99>synchronized (dataQueue) {
<Line#100>            if (!ackQueue.isEmpty()) {
<Line#101>              throw ioe;
<Line#102>            }
<Line#103>          }
<Line#104>        }
<Line#105>        if (shouldStop()) {
<Line#106>          continue;
<Line#107>        }
<Line#108>        endBlock();
<Line#109>      }
<Line#110>      if (progress != null) {
<Line#111>        progress.progress();
<Line#112>      }
<Line#113>      if (artificialSlowdown != 0 && dfsClient.clientRunning) {
<Line#114>        Thread.sleep(artificialSlowdown);
<Line#115>      }
<Line#116>    }
<Line#117> catch (    Throwable e) {
<Line#118>      if (!errorState.isRestartingNode()) {
<Line#119>        if (e instanceof QuotaExceededException) {
<Line#120>          LOG.debug("DataStreamer Quota Exception",e);
<Line#121>        }
<Line#122> else {
<Line#123>          LOG.warn("DataStreamer Exception",e);
<Line#124>        }
<Line#125>      }
<Line#126>      lastException.set(e);
<Line#127>      assert !(e instanceof NullPointerException);
<Line#128>      errorState.setInternalError();
<Line#129>      if (!errorState.isNodeMarked()) {
<Line#130>        streamerClosed=true;
<Line#131>      }
<Line#132>    }
<Line#133> finally {
<Line#134>      if (scope != null) {
<Line#135>        scope.close();
<Line#136>        scope=null;
<Line#137>      }
<Line#138>    }
<Line#139>  }
<Line#140>  closeInternal();
<Line#141>}
Label: <Line#39> LOG.debug("stage={}, {}",stage,this)


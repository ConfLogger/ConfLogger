select <line#> and insert log level and Log verbosity message after <line#>

Query: Target method code:
<Line#0>throws IOException {
<Line#1>Configuration conf = job.getConfiguration();
<Line#2>short replication =
<Line#3>(short) conf.getInt(Job.SUBMIT_REPLICATION,
<Line#4>Job.DEFAULT_SUBMIT_REPLICATION);
<Line#5>
<Line#6>if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {
<Line#7>LOG.warn("Hadoop command-line option parsing not performed. "
<Line#8>+ "Implement the Tool interface and execute your application "
<Line#9>+ "with ToolRunner to remedy this.");
<Line#10>}
<Line#11>
<Line#12>//
<Line#13>// Figure out what fs the JobTracker is using. Copy the
<Line#14>// job to it, under a temporary name. This allows DFS to work,
<Line#15>// and under the local fs also provides UNIX-like object loading
<Line#16>// semantics. (that is, if the job file is deleted right after
<Line#17>// submission, we can still run the submission to completion)
<Line#18>//
<Line#19>
<Line#20>// Create a number of filenames in the JobTracker's fs namespace
<Line#21>LOG.debug("default FileSystem: " + jtFs.getUri());
<Line#22>if (jtFs.exists(submitJobDir)) {
<Line#23>throw new IOException("Not submitting job. Job directory " + submitJobDir
<Line#24>+ " already exists!! This is unexpected.Please check what's there in"
<Line#25>+ " that directory");
<Line#26>}
<Line#27>// Create the submission directory for the MapReduce job.
<Line#28>submitJobDir = jtFs.makeQualified(submitJobDir);
<Line#29>submitJobDir = new Path(submitJobDir.toUri().getPath());
<Line#30>FsPermission mapredSysPerms =
<Line#31>new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);
<Line#32>mkdirs(jtFs, submitJobDir, mapredSysPerms);
<Line#33>
<Line#34>if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,
<Line#35>MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {
<Line#36>disableErasureCodingForPath(submitJobDir);
<Line#37>}
<Line#38>
<Line#39>// Get the resources that have been added via command line arguments in the
<Line#40>// GenericOptionsParser (i.e. files, libjars, archives).
<Line#41>Collection<String> files = conf.getStringCollection("tmpfiles");
<Line#42>Collection<String> libjars = conf.getStringCollection("tmpjars");
<Line#43>Collection<String> archives = conf.getStringCollection("tmparchives");
<Line#44>String jobJar = job.getJar();
<Line#45>
<Line#46>// Merge resources that have been programmatically specified for the shared
<Line#47>// cache via the Job API.
<Line#48>files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));
<Line#49>libjars.addAll(conf.getStringCollection(
<Line#50>MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));
<Line#51>archives.addAll(conf
<Line#52>.getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));
<Line#53>
<Line#54>
<Line#55>Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();
<Line#56>checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);
<Line#57>
<Line#58>Map<String, Boolean> fileSCUploadPolicies =
<Line#59>new LinkedHashMap<String, Boolean>();
<Line#60>Map<String, Boolean> archiveSCUploadPolicies =
<Line#61>new LinkedHashMap<String, Boolean>();
<Line#62>
<Line#63>uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,
<Line#64>fileSCUploadPolicies, statCache);
<Line#65>uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,
<Line#66>fileSCUploadPolicies, statCache);
<Line#67>uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,
<Line#68>archiveSCUploadPolicies, statCache);
<Line#69>uploadJobJar(job, jobJar, submitJobDir, replication, statCache);
<Line#70>addLog4jToDistributedCache(job, submitJobDir);
<Line#71>
<Line#72>// Note, we do not consider resources in the distributed cache for the
<Line#73>// shared cache at this time. Only resources specified via the
<Line#74>// GenericOptionsParser or the jobjar.
<Line#75>Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);
<Line#76>Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);
<Line#77>
<Line#78>// set the timestamps of the archives and files
<Line#79>// set the public/private visibility of the archives and files
<Line#80>ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,
<Line#81>statCache);
<Line#82>// get DelegationToken for cached file
<Line#83>ClientDistributedCacheManager.getDelegationTokens(conf,
<Line#84>job.getCredentials());
<Line#85>}

Example 1:
<Line#1>{
<Line#2>  checkSpecs(job);
<Line#3>  Configuration conf=job.getConfiguration();
<Line#4>  addMRFrameworkToDistributedCache(conf);
<Line#5>  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
<Line#6>  InetAddress ip=InetAddress.getLocalHost();
<Line#7>  if (ip != null) {
<Line#8>    submitHostAddress=ip.getHostAddress();
<Line#9>    submitHostName=ip.getHostName();
<Line#10>    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
<Line#11>    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
<Line#12>  }
<Line#13>  JobID jobId=submitClient.getNewJobID();
<Line#14>  job.setJobID(jobId);
<Line#15>  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
<Line#16>  JobStatus status=null;
<Line#17>  try {
<Line#18>    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
<Line#19>    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
<Line#20>    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
<Line#21>    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
<Line#22>    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
<Line#23>    populateTokenCache(conf,job.getCredentials());
<Line#24>    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
<Line#25>      KeyGenerator keyGen;
<Line#26>      try {
<Line#27>        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
<Line#28>        keyGen.init(SHUFFLE_KEY_LENGTH);
<Line#29>      }
<Line#30> catch (      NoSuchAlgorithmException e) {
<Line#31>        throw new IOException("Error generating shuffle secret key",e);
<Line#32>      }
<Line#33>      SecretKey shuffleKey=keyGen.generateKey();
<Line#34>      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
<Line#35>    }
<Line#36>    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
<Line#37>      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
<Line#38>      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
<Line#39>    }
<Line#40>    copyAndConfigureFiles(job,submitJobDir);
<Line#41>    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
<Line#42>    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
<Line#43>    int maps=writeSplits(job,submitJobDir);
<Line#44>    conf.setInt(MRJobConfig.NUM_MAPS,maps);
<Line#45>    LOG.info("number of splits:" + maps);
<Line#46>    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
<Line#47>    if (maxMaps >= 0 && maxMaps < maps) {
<Line#48>      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
<Line#49>    }
<Line#50>    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
<Line#51>    AccessControlList acl=submitClient.getQueueAdmins(queue);
<Line#52>    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
<Line#53>    TokenCache.cleanUpTokenReferral(conf);
<Line#54>    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
<Line#55>      ArrayList<String> trackingIds=new ArrayList<String>();
<Line#56>      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
<Line#57>        trackingIds.add(t.decodeIdentifier().getTrackingId());
<Line#58>      }
<Line#59>      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
<Line#60>    }
<Line#61>    ReservationId reservationId=job.getReservationId();
<Line#62>    if (reservationId != null) {
<Line#63>      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
<Line#64>    }
<Line#65>    writeConf(conf,submitJobFile);
<Line#66>    printTokens(jobId,job.getCredentials());
<Line#67>    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
<Line#68>    if (status != null) {
<Line#69>      return status;
<Line#70>    }
<Line#71> else {
<Line#72>      throw new IOException("Could not launch job");
<Line#73>    }
<Line#74>  }
<Line#75>  finally {
<Line#76>    if (status == null) {
<Line#77>      LOG.info("Cleaning up the staging area " + submitJobDir);
<Line#78>      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
<Line#79>    }
<Line#80>  }
<Line#81>}
Label: <Line#21> LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir")

Example 2:
<Line#1>{
<Line#2>  checkSpecs(job);
<Line#3>  Configuration conf=job.getConfiguration();
<Line#4>  addMRFrameworkToDistributedCache(conf);
<Line#5>  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
<Line#6>  InetAddress ip=InetAddress.getLocalHost();
<Line#7>  if (ip != null) {
<Line#8>    submitHostAddress=ip.getHostAddress();
<Line#9>    submitHostName=ip.getHostName();
<Line#10>    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
<Line#11>    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
<Line#12>  }
<Line#13>  JobID jobId=submitClient.getNewJobID();
<Line#14>  job.setJobID(jobId);
<Line#15>  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
<Line#16>  JobStatus status=null;
<Line#17>  try {
<Line#18>    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
<Line#19>    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
<Line#20>    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
<Line#21>    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
<Line#22>    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
<Line#23>    populateTokenCache(conf,job.getCredentials());
<Line#24>    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
<Line#25>      KeyGenerator keyGen;
<Line#26>      try {
<Line#27>        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
<Line#28>        keyGen.init(SHUFFLE_KEY_LENGTH);
<Line#29>      }
<Line#30> catch (      NoSuchAlgorithmException e) {
<Line#31>        throw new IOException("Error generating shuffle secret key",e);
<Line#32>      }
<Line#33>      SecretKey shuffleKey=keyGen.generateKey();
<Line#34>      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
<Line#35>    }
<Line#36>    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
<Line#37>      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
<Line#38>      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
<Line#39>    }
<Line#40>    copyAndConfigureFiles(job,submitJobDir);
<Line#41>    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
<Line#42>    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
<Line#43>    int maps=writeSplits(job,submitJobDir);
<Line#44>    conf.setInt(MRJobConfig.NUM_MAPS,maps);
<Line#45>    LOG.info("number of splits:" + maps);
<Line#46>    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
<Line#47>    if (maxMaps >= 0 && maxMaps < maps) {
<Line#48>      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
<Line#49>    }
<Line#50>    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
<Line#51>    AccessControlList acl=submitClient.getQueueAdmins(queue);
<Line#52>    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
<Line#53>    TokenCache.cleanUpTokenReferral(conf);
<Line#54>    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
<Line#55>      ArrayList<String> trackingIds=new ArrayList<String>();
<Line#56>      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
<Line#57>        trackingIds.add(t.decodeIdentifier().getTrackingId());
<Line#58>      }
<Line#59>      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
<Line#60>    }
<Line#61>    ReservationId reservationId=job.getReservationId();
<Line#62>    if (reservationId != null) {
<Line#63>      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
<Line#64>    }
<Line#65>    writeConf(conf,submitJobFile);
<Line#66>    printTokens(jobId,job.getCredentials());
<Line#67>    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
<Line#68>    if (status != null) {
<Line#69>      return status;
<Line#70>    }
<Line#71> else {
<Line#72>      throw new IOException("Could not launch job");
<Line#73>    }
<Line#74>  }
<Line#75>  finally {
<Line#76>    if (status == null) {
<Line#77>      LOG.info("Cleaning up the staging area " + submitJobDir);
<Line#78>      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
<Line#79>    }
<Line#80>  }
<Line#81>}
Label: <Line#38> LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled")

Example 3:
<Line#1>{
<Line#2>  checkSpecs(job);
<Line#3>  Configuration conf=job.getConfiguration();
<Line#4>  addMRFrameworkToDistributedCache(conf);
<Line#5>  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
<Line#6>  InetAddress ip=InetAddress.getLocalHost();
<Line#7>  if (ip != null) {
<Line#8>    submitHostAddress=ip.getHostAddress();
<Line#9>    submitHostName=ip.getHostName();
<Line#10>    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
<Line#11>    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
<Line#12>  }
<Line#13>  JobID jobId=submitClient.getNewJobID();
<Line#14>  job.setJobID(jobId);
<Line#15>  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
<Line#16>  JobStatus status=null;
<Line#17>  try {
<Line#18>    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
<Line#19>    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
<Line#20>    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
<Line#21>    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
<Line#22>    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
<Line#23>    populateTokenCache(conf,job.getCredentials());
<Line#24>    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
<Line#25>      KeyGenerator keyGen;
<Line#26>      try {
<Line#27>        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
<Line#28>        keyGen.init(SHUFFLE_KEY_LENGTH);
<Line#29>      }
<Line#30> catch (      NoSuchAlgorithmException e) {
<Line#31>        throw new IOException("Error generating shuffle secret key",e);
<Line#32>      }
<Line#33>      SecretKey shuffleKey=keyGen.generateKey();
<Line#34>      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
<Line#35>    }
<Line#36>    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
<Line#37>      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
<Line#38>      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
<Line#39>    }
<Line#40>    copyAndConfigureFiles(job,submitJobDir);
<Line#41>    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
<Line#42>    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
<Line#43>    int maps=writeSplits(job,submitJobDir);
<Line#44>    conf.setInt(MRJobConfig.NUM_MAPS,maps);
<Line#45>    LOG.info("number of splits:" + maps);
<Line#46>    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
<Line#47>    if (maxMaps >= 0 && maxMaps < maps) {
<Line#48>      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
<Line#49>    }
<Line#50>    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
<Line#51>    AccessControlList acl=submitClient.getQueueAdmins(queue);
<Line#52>    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
<Line#53>    TokenCache.cleanUpTokenReferral(conf);
<Line#54>    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
<Line#55>      ArrayList<String> trackingIds=new ArrayList<String>();
<Line#56>      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
<Line#57>        trackingIds.add(t.decodeIdentifier().getTrackingId());
<Line#58>      }
<Line#59>      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
<Line#60>    }
<Line#61>    ReservationId reservationId=job.getReservationId();
<Line#62>    if (reservationId != null) {
<Line#63>      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
<Line#64>    }
<Line#65>    writeConf(conf,submitJobFile);
<Line#66>    printTokens(jobId,job.getCredentials());
<Line#67>    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
<Line#68>    if (status != null) {
<Line#69>      return status;
<Line#70>    }
<Line#71> else {
<Line#72>      throw new IOException("Could not launch job");
<Line#73>    }
<Line#74>  }
<Line#75>  finally {
<Line#76>    if (status == null) {
<Line#77>      LOG.info("Cleaning up the staging area " + submitJobDir);
<Line#78>      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
<Line#79>    }
<Line#80>  }
<Line#81>}
Label: <Line#42> LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir))

Example 4:
<Line#1>{
<Line#2>  checkSpecs(job);
<Line#3>  Configuration conf=job.getConfiguration();
<Line#4>  addMRFrameworkToDistributedCache(conf);
<Line#5>  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
<Line#6>  InetAddress ip=InetAddress.getLocalHost();
<Line#7>  if (ip != null) {
<Line#8>    submitHostAddress=ip.getHostAddress();
<Line#9>    submitHostName=ip.getHostName();
<Line#10>    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
<Line#11>    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
<Line#12>  }
<Line#13>  JobID jobId=submitClient.getNewJobID();
<Line#14>  job.setJobID(jobId);
<Line#15>  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
<Line#16>  JobStatus status=null;
<Line#17>  try {
<Line#18>    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
<Line#19>    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
<Line#20>    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
<Line#21>    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
<Line#22>    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
<Line#23>    populateTokenCache(conf,job.getCredentials());
<Line#24>    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
<Line#25>      KeyGenerator keyGen;
<Line#26>      try {
<Line#27>        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
<Line#28>        keyGen.init(SHUFFLE_KEY_LENGTH);
<Line#29>      }
<Line#30> catch (      NoSuchAlgorithmException e) {
<Line#31>        throw new IOException("Error generating shuffle secret key",e);
<Line#32>      }
<Line#33>      SecretKey shuffleKey=keyGen.generateKey();
<Line#34>      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
<Line#35>    }
<Line#36>    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
<Line#37>      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
<Line#38>      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
<Line#39>    }
<Line#40>    copyAndConfigureFiles(job,submitJobDir);
<Line#41>    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
<Line#42>    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
<Line#43>    int maps=writeSplits(job,submitJobDir);
<Line#44>    conf.setInt(MRJobConfig.NUM_MAPS,maps);
<Line#45>    LOG.info("number of splits:" + maps);
<Line#46>    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
<Line#47>    if (maxMaps >= 0 && maxMaps < maps) {
<Line#48>      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
<Line#49>    }
<Line#50>    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
<Line#51>    AccessControlList acl=submitClient.getQueueAdmins(queue);
<Line#52>    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
<Line#53>    TokenCache.cleanUpTokenReferral(conf);
<Line#54>    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
<Line#55>      ArrayList<String> trackingIds=new ArrayList<String>();
<Line#56>      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
<Line#57>        trackingIds.add(t.decodeIdentifier().getTrackingId());
<Line#58>      }
<Line#59>      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
<Line#60>    }
<Line#61>    ReservationId reservationId=job.getReservationId();
<Line#62>    if (reservationId != null) {
<Line#63>      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
<Line#64>    }
<Line#65>    writeConf(conf,submitJobFile);
<Line#66>    printTokens(jobId,job.getCredentials());
<Line#67>    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
<Line#68>    if (status != null) {
<Line#69>      return status;
<Line#70>    }
<Line#71> else {
<Line#72>      throw new IOException("Could not launch job");
<Line#73>    }
<Line#74>  }
<Line#75>  finally {
<Line#76>    if (status == null) {
<Line#77>      LOG.info("Cleaning up the staging area " + submitJobDir);
<Line#78>      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
<Line#79>    }
<Line#80>  }
<Line#81>}
Label: <Line#45> LOG.info("number of splits:" + maps)

Example 5:
<Line#1>{
<Line#2>  checkSpecs(job);
<Line#3>  Configuration conf=job.getConfiguration();
<Line#4>  addMRFrameworkToDistributedCache(conf);
<Line#5>  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
<Line#6>  InetAddress ip=InetAddress.getLocalHost();
<Line#7>  if (ip != null) {
<Line#8>    submitHostAddress=ip.getHostAddress();
<Line#9>    submitHostName=ip.getHostName();
<Line#10>    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
<Line#11>    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
<Line#12>  }
<Line#13>  JobID jobId=submitClient.getNewJobID();
<Line#14>  job.setJobID(jobId);
<Line#15>  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
<Line#16>  JobStatus status=null;
<Line#17>  try {
<Line#18>    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
<Line#19>    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
<Line#20>    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
<Line#21>    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
<Line#22>    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
<Line#23>    populateTokenCache(conf,job.getCredentials());
<Line#24>    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
<Line#25>      KeyGenerator keyGen;
<Line#26>      try {
<Line#27>        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
<Line#28>        keyGen.init(SHUFFLE_KEY_LENGTH);
<Line#29>      }
<Line#30> catch (      NoSuchAlgorithmException e) {
<Line#31>        throw new IOException("Error generating shuffle secret key",e);
<Line#32>      }
<Line#33>      SecretKey shuffleKey=keyGen.generateKey();
<Line#34>      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
<Line#35>    }
<Line#36>    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
<Line#37>      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
<Line#38>      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
<Line#39>    }
<Line#40>    copyAndConfigureFiles(job,submitJobDir);
<Line#41>    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
<Line#42>    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
<Line#43>    int maps=writeSplits(job,submitJobDir);
<Line#44>    conf.setInt(MRJobConfig.NUM_MAPS,maps);
<Line#45>    LOG.info("number of splits:" + maps);
<Line#46>    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
<Line#47>    if (maxMaps >= 0 && maxMaps < maps) {
<Line#48>      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
<Line#49>    }
<Line#50>    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
<Line#51>    AccessControlList acl=submitClient.getQueueAdmins(queue);
<Line#52>    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
<Line#53>    TokenCache.cleanUpTokenReferral(conf);
<Line#54>    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
<Line#55>      ArrayList<String> trackingIds=new ArrayList<String>();
<Line#56>      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
<Line#57>        trackingIds.add(t.decodeIdentifier().getTrackingId());
<Line#58>      }
<Line#59>      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
<Line#60>    }
<Line#61>    ReservationId reservationId=job.getReservationId();
<Line#62>    if (reservationId != null) {
<Line#63>      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
<Line#64>    }
<Line#65>    writeConf(conf,submitJobFile);
<Line#66>    printTokens(jobId,job.getCredentials());
<Line#67>    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
<Line#68>    if (status != null) {
<Line#69>      return status;
<Line#70>    }
<Line#71> else {
<Line#72>      throw new IOException("Could not launch job");
<Line#73>    }
<Line#74>  }
<Line#75>  finally {
<Line#76>    if (status == null) {
<Line#77>      LOG.info("Cleaning up the staging area " + submitJobDir);
<Line#78>      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
<Line#79>    }
<Line#80>  }
<Line#81>}
Label: <Line#77> LOG.info("Cleaning up the staging area " + submitJobDir)

